{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "J0q9NP3X9cgG",
        "UjBTFl-d-G2H",
        "mL4_nJy8MaZN",
        "RtQ9NnZGNFb_",
        "iGH6jlP8O5qb",
        "HTGZMH3oPfdP",
        "Gc4eR7WNQQ5v",
        "pAAYYsNFjtR9",
        "MeqQ17rWlteD",
        "WOviH2EwnfPG",
        "A8hvDiuon9BO"
      ],
      "authorship_tag": "ABX9TyMWf/CHAUxiO3HzJaan5sPT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jay05Hawk/NLP-Everything/blob/main/NLP_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vdoiKyZai0v"
      },
      "outputs": [],
      "source": [
        "# pip install -U spacy\n",
        "# pip install -U spacy-lookups-data\n",
        "# python -m spacy download en_core_web_sm\n",
        "# python -m spacy download en_core_web_md\n",
        "# python -m spacy download en_core_web_lg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://textblob.readthedocs.io/en/dev/api_reference.html\n",
        "\n",
        "https://www.loc.gov/standards/iso639-2/php/code_list.php  $\\color{red}{\\text{for language code}}$"
      ],
      "metadata": {
        "id": "eBqs4qKwNnzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS"
      ],
      "metadata": {
        "id": "0bO32XxVbQUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('twitter16m.csv', encoding = 'latin1', header = None)"
      ],
      "metadata": {
        "id": "6Mq7KSD0bysI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "AI55tfYab0h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[[5, 0]]\n"
      ],
      "metadata": {
        "id": "xqLh-XPxb0mM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = ['twitts', 'sentiment']\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Y5X6pTbGb0p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "SrQO1MACb0ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_map = {0: 'negative', 4: 'positive'}"
      ],
      "metadata": {
        "id": "DNeRE9kwb0w3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Word Counts}}$\n",
        "In this step, we are splitting the sentences into words using split() function which converts the sentence into the list and on top of that we are using len() function to calculate the number of token or words."
      ],
      "metadata": {
        "id": "5k0xCwB8cWZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['word_counts'] = df['twitts'].apply(lambda x: len(str(x).split()))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "7MmxrAwSb1Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Characters Count}}$\n",
        "In this step, we are using len() function to calculate the number characters inside each sentences."
      ],
      "metadata": {
        "id": "2rpY0sLLddob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['char_counts'] = df['twitts'].apply(lambda x: len(x))\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "BUj2FW02b1Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Average Word Length}}$\n",
        "In this step, we have created a function get_avg_word_len() in which we are calculating the average word length inside each sentences."
      ],
      "metadata": {
        "id": "sDPX17BGdzqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_avg_word_len(x):\n",
        "    words = x.split()\n",
        "    word_len = 0\n",
        "    for word in words:\n",
        "        word_len = word_len + len(word)\n",
        "    return word_len/len(words) # != len(x)/len(words)"
      ],
      "metadata": {
        "id": "MYalJXgKb1OX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['avg_word_len'] = df['twitts'].apply(lambda x: get_avg_word_len(x))\n",
        "len('this is nlp lesson')/4"
      ],
      "metadata": {
        "id": "RDX-ttROb1R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "UwwELbBPb1VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Stop Words Count}}$\n",
        "In this section, we are calculating the number of stop words for each sentences."
      ],
      "metadata": {
        "id": "0U707j6ceVkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(STOP_WORDS)"
      ],
      "metadata": {
        "id": "_7d-FVK-b1ZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = 'this is text data'\n",
        "x.split()\n",
        "len([t for t in x.split() if t in STOP_WORDS])\n"
      ],
      "metadata": {
        "id": "FD_kl7I2hAAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['stop_words_len'] = df['twitts'].apply(lambda x: len([t for t in x.split() if t in STOP_WORDS]))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ecMwKeIehA0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Count #HashTags and @Mentions}}$\n",
        "In this section, we are calculating the number of words staring with Hashtags and @."
      ],
      "metadata": {
        "id": "L9OhXrV_hqhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = 'this #hashtag and this is @mention'\n",
        "x = x.split() \n",
        "x"
      ],
      "metadata": {
        "id": "vmaSZA0thBQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['hashtags_count'] = df['twitts'].apply(lambda x: len([t for t in x.split() if t.startswith('#')]))\n",
        "df['mentions_count'] = df['twitts'].apply(lambda x: len([t for t in x.split() if t.startswith('@')]))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "0w-1W4oXhBcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##If numeric digits are present in twitts\n",
        "In this section, we are calculating the number of digits in each sentences."
      ],
      "metadata": {
        "id": "J0q9NP3X9cgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['numerics_count'] = df['twitts'].apply(lambda x: len([t for t in x.split() if t.isdigit()]))\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "K5Mprd6vhBks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{UPPER case}}$ words count\n",
        "In this section, we are calculating the number of UPPERcase words in each sentences if length is more than 3."
      ],
      "metadata": {
        "id": "Fxr2hEvw9tB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['upper_counts'] = df['twitts'].apply(lambda x: len([t for t in x.split() if t.isupper() and len(x)>3]))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "YoVoB6fehBqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Preprocessing and Cleaning}}$\n",
        "In this section, we are converting the words to LOWERcase words in each sentences."
      ],
      "metadata": {
        "id": "Wb5bi5DL-EmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lower case conversion"
      ],
      "metadata": {
        "id": "UjBTFl-d-G2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['twitts'] = df['twitts'].apply(lambda x: x.lower())\n",
        "df.head(2)"
      ],
      "metadata": {
        "id": "fz7dYVpZ94jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Contraction to Expansion}}$\n",
        "In this section, we are converting all short words to their respective fullwords based on the words defined in the dictionary and using function cont_to_exp()."
      ],
      "metadata": {
        "id": "5sxKqIbzFCZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = \"i don't know what you want, can't, he'll, i'd\""
      ],
      "metadata": {
        "id": "gOsVovRXEPdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contractions = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how does\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so is\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\" u \": \" you \",\n",
        "\" ur \": \" your \",\n",
        "\" n \": \" and \"}"
      ],
      "metadata": {
        "id": "xINSbXFiE7Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cont_to_exp(x):\n",
        "    if type(x) is str:\n",
        "        for key in contractions:\n",
        "            value = contractions[key]\n",
        "            x = x.replace(key, value)\n",
        "        return x\n",
        "    else:\n",
        "        return x"
      ],
      "metadata": {
        "id": "IJ0WAQLKFb8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = \"hi, i'd be happy\"\n",
        "cont_to_exp(x)"
      ],
      "metadata": {
        "id": "GXMsCm-eFzy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df['twitts'] = df['twitts'].apply(lambda x: cont_to_exp(x))"
      ],
      "metadata": {
        "id": "4LjtgYurF0C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "E7uQ6vgCF0MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Count and Remove Emails}}$\n",
        "In this section, we are removing as well as counting the emails."
      ],
      "metadata": {
        "id": "OoVGBeaQGeE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "8KO4JDvrF0sI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = 'hi my email me at email@email.com another@email.com'\n",
        "re.findall(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', x)"
      ],
      "metadata": {
        "id": "eg-waTw9F00-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['emails'] = df['twitts'].apply(lambda x: re.findall(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', x))\n",
        "df['emails_count'] = df['emails'].apply(lambda x: len(x))\n",
        "df[df['emails_count']>0].head()"
      ],
      "metadata": {
        "id": "rE8D0iGGGwtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove emails\n",
        "df['twitts'] = df['twitts'].apply(lambda x: re.sub(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', '', x))\n",
        "df[df['emails_count']>0].head()"
      ],
      "metadata": {
        "id": "NfcwsmhCGw35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Count URLs and Remove it}}$\n",
        "In this section, we are removing as well as counting the URLs using regex functions."
      ],
      "metadata": {
        "id": "pK3GcNprKwUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = 'hi, to watch more visit https://youtube.com/jayk'"
      ],
      "metadata": {
        "id": "zolezNoVGw_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', x)"
      ],
      "metadata": {
        "id": "0CgvrXAJGxG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find urls\n",
        "df['urls_flag'] = df['twitts'].apply(lambda x: len(re.findall(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', x)))\n",
        "#remmove urls\n",
        "re.sub(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', 'url removed', x)"
      ],
      "metadata": {
        "id": "PL0aGL5MLFoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['twitts'] = df['twitts'].apply(lambda x: re.sub(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '', x))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "PwjoTWMcLiSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[0]['twitts']"
      ],
      "metadata": {
        "id": "SAV8XKKiMFHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Remove RT\n",
        "In this section, we are removing retweet characters."
      ],
      "metadata": {
        "id": "mL4_nJy8MaZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['twitts'] = df['twitts'].apply(lambda x: re.sub('RT', \"\", x))\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "76_o6lU6MQ8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Special Chars removal or punctuation removal\n",
        "In this section, we are removing special characters and punctuations"
      ],
      "metadata": {
        "id": "RtQ9NnZGNFb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['twitts'] = df['twitts'].apply(lambda x: re.sub('[^A-Z a-z 0-9-]+', '', x))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "6RmHsgMUMcc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Remove multiple spaces \"hi hello \"}}$\n",
        "In this section, we are removing the multiple spaces."
      ],
      "metadata": {
        "id": "t4D6cn_WNgvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = 'thanks    for    watching and    please    like this video'\n",
        "\" \".join(x.split())"
      ],
      "metadata": {
        "id": "XttLGxLoMdRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['twitts'] = df['twitts'].apply(lambda x: \" \".join(x.split()))\n",
        "df.head(2)\n"
      ],
      "metadata": {
        "id": "F0Dd6J9BMdlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Remove HTML tags}}$\n",
        "In this section, we are removing the HTML tags."
      ],
      "metadata": {
        "id": "qtGODD9bN1Wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "x = '<html><h2>Thanks for watching</h2></html>'"
      ],
      "metadata": {
        "id": "oyykUXqwMd19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BeautifulSoup(x, 'lxml').get_text()"
      ],
      "metadata": {
        "id": "jdGEdMM1OBe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df['twitts'] = df['twitts'].apply(lambda x: BeautifulSoup(x, 'lxml').get_text())"
      ],
      "metadata": {
        "id": "qC3_hA22OEh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Remove Accented Chars}}$\n",
        "In this section, we are removing the accented characters."
      ],
      "metadata": {
        "id": "MNHOMd7oOhfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "x = 'Áccěntěd těxt'"
      ],
      "metadata": {
        "id": "MqAVnQRdOMhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_accented_chars(x):\n",
        "    x = unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "remove_accented_chars(x)"
      ],
      "metadata": {
        "id": "tW9Z-b1_Osad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{SpaCy and NLP}}$\n",
        "##Remove Stop Words\n",
        "In this section, we are removing the stop words from text document."
      ],
      "metadata": {
        "id": "iGH6jlP8O5qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "\n",
        "\n",
        "x = 'this is stop words removal code is a the an how what'"
      ],
      "metadata": {
        "id": "VeQPySyvOyRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\" \".join([t for t in x.split() if t not in STOP_WORDS])"
      ],
      "metadata": {
        "id": "FXRqHSzZPC9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['twitts'] = df['twitts'].apply(lambda x: \" \".join([t for t in x.split() if t not in STOP_WORDS]))\n",
        "\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "hiFzCTeCPD05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Convert into base or root form of word\n",
        "In this section, we are converting the words to their forms."
      ],
      "metadata": {
        "id": "HTGZMH3oPfdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "5OD8HgSYP9ES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = 'kenichan dived times ball managed save 50 rest'\n",
        "# dive = dived, time = times, manage = managed\n",
        "# x = 'i you he she they is am are'"
      ],
      "metadata": {
        "id": "dC4Drp1CPEGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_to_base(x):\n",
        "    x_list = []\n",
        "    doc = nlp(x)\n",
        "    \n",
        "    for token in doc:\n",
        "        lemma = str(token.lemma_)\n",
        "        if lemma == '-PRON-' or lemma == 'be':\n",
        "            lemma = token.text\n",
        "        x_list.append(lemma)\n",
        "    print(\" \".join(x_list))"
      ],
      "metadata": {
        "id": "U-hxE_FKPEV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_to_base(x)"
      ],
      "metadata": {
        "id": "EBLMfp8yPElH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Common words removal\n",
        "In this section, we are removing top 20 most occured word from text corpus."
      ],
      "metadata": {
        "id": "Gc4eR7WNQQ5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join(df.head()['twitts'])\n",
        "\n",
        "\n",
        "'switchfoot - awww bummer shoulda got david carr day d upset update facebook texting cry result school today blah kenichan dived times ball managed save 50 rest bounds body feels itchy like fire nationwideclass behaving mad'"
      ],
      "metadata": {
        "id": "m5LOeMIiPE09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ' '.join(df['twitts'])\n",
        "text = text.split()\n",
        "freq_comm = pd.Series(text).value_counts()\n",
        "f20 = freq_comm[:20]\n",
        "f20"
      ],
      "metadata": {
        "id": "GzURq0OLQkzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['twitts'] = df['twitts'].apply(lambda x: \" \".join([t for t in x.split() if t not in f20]))"
      ],
      "metadata": {
        "id": "4wcjkU2KQotp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Rare words removal}}$\n",
        "In this section, we are removing least 20 most occured word from text corpus."
      ],
      "metadata": {
        "id": "V_MLgUwpQ6V5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rare20 = freq_comm[-20:]"
      ],
      "metadata": {
        "id": "2MJIg8mZQ00_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rare20"
      ],
      "metadata": {
        "id": "OEORk2sa1eY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rare = freq_comm[freq_comm.values == 1]\n",
        "rare"
      ],
      "metadata": {
        "id": "q60aqiaq1xB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['twitts'] = df['twitts'].apply(lambda x: ' '.join([t for t in x.split() if t not in rare20]))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "nKS9TRx610qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Word Cloud Visualization}}$\n",
        "In this section, we are visualizing the text corpus using library WordCloud."
      ],
      "metadata": {
        "id": "sgggTYsv2cYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install wordcloud\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "H3broX-I1iG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = ' '.join(text[:20000])\n",
        "x"
      ],
      "metadata": {
        "id": "WMp-00-82RxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y=\"\"\"jaykishan jaykishan \n",
        "# jaykishan jaykishan jaykishan\n",
        "#  jaykishan jaykishan jaykishan\n",
        "#   jaykishan jaykishan jaykishan \n",
        "#   jaykishan jaykishan jaykishan \n",
        "#   jaykishan jaykishan jaykishan jaykishan jaykishan kishankumar \n",
        "#   nikhil chanda\n",
        "#   nikhil chandanikhil chanda\n",
        "#   sakshi sandeep\n",
        "# \"\"\"\n"
      ],
      "metadata": {
        "id": "KTs-qv063pu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "id": "bW9qruwa2SEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wc = WordCloud(width = 800, height=400).generate(y)\n",
        "wc = WordCloud(width = 800, height=400).generate(x)\n",
        "plt.imshow(wc)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gO0MhV_C2SVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Spelling Correction\n",
        "In this section, we are correcting the spelling of each words."
      ],
      "metadata": {
        "id": "GcZZ13Nc5g8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U textblob\n",
        "# !python -m textblob.download_corpora\n",
        "from textblob import TextBlob\n",
        "x = 'tanks forr waching this vidio carri'"
      ],
      "metadata": {
        "id": "H43WEOBt2SiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = TextBlob(x).correct()\n",
        "x"
      ],
      "metadata": {
        "id": "8ozOy1IM2SvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Tokenization}}$\n",
        "Tokenization is all about breaking the sentences into individual words."
      ],
      "metadata": {
        "id": "SA1YNHPu7HtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "x = 'thanks#watching this video. please like it'\n",
        "TextBlob(x).words\n",
        "\n",
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "# Sentences=nltk.sent_tokenize(x)\n"
      ],
      "metadata": {
        "id": "KtsVZQKe513k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "id": "0_MPOhEv-ifZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(x)\n",
        "for token in doc:\n",
        "    print(token)"
      ],
      "metadata": {
        "id": "9jHiyHZ578MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Lemmatization}}$\n",
        "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word."
      ],
      "metadata": {
        "id": "Hp52wsJP9pnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.stem import WordNetLemmatizer\n",
        "# lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "x = 'runs run running ran stopped'"
      ],
      "metadata": {
        "id": "pQkKBTSi9YC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(\"runs run running ran :\", lemmatizer.lemmatize(x))"
      ],
      "metadata": {
        "id": "Qw0lbUPzBmzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for token in x.split():\n",
        "#     print(Word(token).lemmatize())"
      ],
      "metadata": {
        "id": "PfcZY7mb9sLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(x)\n",
        "for token in doc:\n",
        "    print(token.lemma_)"
      ],
      "metadata": {
        "id": "JrQVGH499s5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Detect Entities using NER of SpaCy}}$\n",
        "Named Entity Recognition (NER) is a standard NLP problem which involves spotting named entities (people, places, organizations etc.) from a chunk of text, and classifying them into a predefined set of categories. Some of the practical applications of NER include:\n",
        "\n",
        "Scanning news articles for the people, organizations and locations reported.\n",
        "Providing concise features for search optimization: instead of searching the entire content, one may simply search for the major entities involved.\n",
        "Quickly retrieving geographical locations talked about in Twitter posts."
      ],
      "metadata": {
        "id": "UHwfU6J4DcWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = \"Breaking News: Donald Trump, the president of the USA is looking to sign a deal to mine the moon\""
      ],
      "metadata": {
        "id": "vwYUkyiQ9tAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(x)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text + ' - ' + ent.label_ + ' - ' + str(spacy.explain(ent.label_)))"
      ],
      "metadata": {
        "id": "OBDa5NQi9tKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(doc, style =\"ent\")\n",
        "#displacy.serve(doc, style=\"ent\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ca3n7vzv9taV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Detecting Nouns}}$\n",
        "In this section, we are detecting nouns."
      ],
      "metadata": {
        "id": "0WZQ3IfMJZFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for noun in doc.noun_chunks:\n",
        "    print(noun)"
      ],
      "metadata": {
        "id": "3XJ_sxgk9tjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Translation and Language Detection}}$"
      ],
      "metadata": {
        "id": "iZLRB1FpJzM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tb = TextBlob(x)\n"
      ],
      "metadata": {
        "id": "6cj-Z7nU9tzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tb.detect_language()"
      ],
      "metadata": {
        "id": "jN12s5tD9t74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tb.translate(to='bn')"
      ],
      "metadata": {
        "id": "wu1Jcf8X9uCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Use inbuilt sentiment classifier\n",
        "TextBlob library also comes with a NaiveBayesAnalyzer, Naive Bayes is a commonly used machine learning text-classification algorithm."
      ],
      "metadata": {
        "id": "PQmQH8-iRw3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob.sentiments import NaiveBayesAnalyzer"
      ],
      "metadata": {
        "id": "nEnOftFQRyyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = 'we all stands together to fight with corona virus. we will win together'\n",
        "tb = TextBlob(x, analyzer=NaiveBayesAnalyzer())"
      ],
      "metadata": {
        "id": "b0o13bltR06l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tb.sentiment"
      ],
      "metadata": {
        "id": "dGKT7dyGR0sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Advanced Text Processing}}$\n",
        "##$\\color{green}{\\text{N-Grams}}$\n",
        "An N-gram means a sequence of N words. So for example, “range rover ” is a 2-gram (a bigram), “$\\color{green}{\\text{B}}$avarian $\\color{green}{\\text{M}}$otor $\\color{green}{\\text{W}}$orks” is a 3-gram(trigram), and “choice your favorate car” is a 4-gram . Well, that wasn’t very interesting or exciting. True, but we still have to look at the probability used with n-grams, which is quite interesting."
      ],
      "metadata": {
        "id": "--DHAJOwTtfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = 'thanks for watching'\n",
        "tb = TextBlob(x)\n"
      ],
      "metadata": {
        "id": "CqmNz0xtR0fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tb.ngrams(3)"
      ],
      "metadata": {
        "id": "3FRNJK19R0SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words  $\\color{red}{\\text{BoW}}$\n",
        "In this section, we are going to discuss a Natural Language Processing technique of text modeling known as the Bag of Words model. Whenever we apply any algorithm in NLP, it works on numbers. We cannot directly feed our text into that algorithm. Hence, the Bag of Words model is used to preprocess the text by converting it into a bag of words, which keeps a count of the total occurrences of most frequently used words.\n",
        "\n",
        "This model can be visualized using a table, which contains the count of words corresponding to the word itself."
      ],
      "metadata": {
        "id": "qDeBBsQAUC2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = ['this is first sentence this is', 'this is second', 'this is second last','this is last']\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "x5XQtj7XR0D6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(ngram_range=(1,1))\n",
        "text_counts = cv.fit_transform(x)"
      ],
      "metadata": {
        "id": "ZfVqvedaRz0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_counts.toarray()"
      ],
      "metadata": {
        "id": "axIJ3MQZRzk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.get_feature_names()"
      ],
      "metadata": {
        "id": "f4-PDvqwRzUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow = pd.DataFrame(text_counts.toarray(), columns = cv.get_feature_names())\n",
        "bow"
      ],
      "metadata": {
        "id": "jMAYYnZ_W1rQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Term Frequency}}$\n",
        "Term frequency (TF) often used in Text Mining, NLP, and Information Retrieval tells you how frequently a term occurs in a document. In the context of natural language, terms correspond to words or phrases. Since every document is different in length, it is possible that a term would appear more often in longer documents than shorter ones. Thus, term frequency is often divided by the total number of terms in the document as a way of $\\color{green}{\\text{normalization}}$.\n",
        "\n",
        "$\\color{blue}{\\text{TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)}}$."
      ],
      "metadata": {
        "id": "8jVd8rYJXSPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow.shape"
      ],
      "metadata": {
        "id": "nS20iZfrW2nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf = bow.copy()"
      ],
      "metadata": {
        "id": "nf6LqvMiW23A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in enumerate(tf.iterrows()):\n",
        "    for col in row[1].index:\n",
        "        tf.loc[index, col] = tf.loc[index, col]/sum(row[1].values)\n",
        "\n",
        "\n",
        "tf"
      ],
      "metadata": {
        "id": "1WFlVpNYW3C7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Inverse Document Frequency IDF}}$\n",
        "$\\color{blue}{\\text{Inverse Document Frequency (IDF) is a weight indicating how commonly a word is used. The more frequent its usage across documents, the lower its score. The lower the score, the less important the word becomes.}}$\n",
        "\n",
        "For example, the word the appears in almost all English texts and would thus have a very low IDF score as it carries very little “topic” information. In contrast, if you take the word coffee, while it is common, it’s not used as widely as the word the. Thus, coffee would have a higher IDF score than the.\n",
        "For example, the word the appears in almost all English texts and would thus have a very low IDF score as it carries very little “topic” information. In contrast, if you take the word coffee, while it is common, it’s not used as widely as the word the. Thus, coffee would have a higher IDF score than the.\n",
        "\n",
        "$\\color{blue}{\\text{idf = log( (1 + N)/(n + 1)) + 1 used in sklearn when smooth_idf = True}}$\n",
        "where, $\\color{green}{\\text{N}}$ is the total number of rows and  $\\color{red}{\\text{n}}$ is the number of rows in which the word was present."
      ],
      "metadata": {
        "id": "JWbmKuWadhtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x_df = pd.DataFrame(x, columns=['words'])\n",
        "x_df"
      ],
      "metadata": {
        "id": "PCqV7SvOW3Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow"
      ],
      "metadata": {
        "id": "Zi0Xfo0fW3ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = bow.shape[0]\n",
        "N"
      ],
      "metadata": {
        "id": "yeW4OHJZW3us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bb = bow.astype('bool')\n",
        "bb"
      ],
      "metadata": {
        "id": "AdPzPsBNW39z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bb['is'].sum()"
      ],
      "metadata": {
        "id": "C9Seaoi4W4Km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = bb.columns\n",
        "cols"
      ],
      "metadata": {
        "id": "uhGS2bmSW4Zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nz = []\n",
        "for col in cols:\n",
        "    nz.append(bb[col].sum())"
      ],
      "metadata": {
        "id": "JdVgWd7MW4n_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nz"
      ],
      "metadata": {
        "id": "iET30jNgW43m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idf = []\n",
        "for index, col in enumerate(cols):\n",
        "    idf.append(np.log((N + 1)/(nz[index] + 1)) + 1)\n",
        "\n",
        "\n",
        "idf"
      ],
      "metadata": {
        "id": "1rB51dHVW5Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{TF-IDF}}$\n",
        "TF-IDF which stands for Term Frequency – Inverse Document Frequency. It is one of the most important techniques used for information retrieval to represent how important a specific word or phrase is to a given document. Let’s take an example, we have a string or Bag of Words (BOW) and we have to extract information from it, then we can use this approach.\n",
        "\n",
        "The tf-idf value increases in proportion to the number of times a word appears in the document but is often offset by the frequency of the word in the corpus, which helps to adjust with respect to the fact that $\\color{red}{\\text{some words appear more frequently in general.}}$\n",
        "\n",
        "TF-IDF use two statistical methods, first is Term Frequency and the other is Inverse Document Frequency. Term frequency refers to the total number of times a given term t appears in the document doc against (per) the total number of all words in the document and The inverse document frequency measure of how much information the word provides. It measures the weight of a given word in the entire document. IDF show how common or rare a given word is across all documents."
      ],
      "metadata": {
        "id": "Op4C6-YVf_EP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "x_tfidf = tfidf.fit_transform(x_df['words'])\n",
        "\n",
        "\n",
        "x_tfidf.toarray()"
      ],
      "metadata": {
        "id": "_3HIjJJjW5T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf.idf_"
      ],
      "metadata": {
        "id": "2lHC-FJ0W5hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idf"
      ],
      "metadata": {
        "id": "iQCWi0Z3hE5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Word Embeddings}}$\n",
        "Word Embedding is a language modeling technique used for mapping words to vectors of real numbers. It represents words or phrases in vector space with several dimensions. Word embeddings can be generated using various methods like neural networks, co-occurrence matrix, probabilistic models, etc.\n",
        "\n",
        "#$\\color{orange}{\\text{SpaCy}}$ $\\color{blue}{\\text{Word2Vec}}$ "
      ],
      "metadata": {
        "id": "hukyGfYEhViR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "\n",
        "doc = nlp('thank you! dog cat lion tiger')"
      ],
      "metadata": {
        "id": "Hw3IkWfthF3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "    print(token.text, token.has_vector)"
      ],
      "metadata": {
        "id": "Hb1v-MwZhGNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token.vector.shape"
      ],
      "metadata": {
        "id": "V7g4xao5hGfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp('cat').vector.shape"
      ],
      "metadata": {
        "id": "b-iZOLAwhGwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token1 in doc:\n",
        "    for token2 in doc:\n",
        "        print(token1.text, token2.text, token1.similarity(token2))\n",
        "    print()"
      ],
      "metadata": {
        "id": "gEt2nO41hHBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{dark-blue}{\\text{Machine Learning Models for Text Classification}}$ \n",
        "$\\color{purple}{\\text{BOW}}$"
      ],
      "metadata": {
        "id": "pAAYYsNFjtR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "ndAe-HABhHS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sampling the number of rows\n",
        "\n",
        "df0 = df[df['sentiment']==0].sample(55)\n",
        "df4 = df[df['sentiment']==4].sample(45)"
      ],
      "metadata": {
        "id": "MowfLM3_hHj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfr = df0.append(df4)\n",
        "dfr.shape"
      ],
      "metadata": {
        "id": "mLQyMscnhH06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removing the twitts,sentiment and emails columns\n",
        "\n",
        "dfr_feat = dfr.drop(labels=['twitts','sentiment','emails'], axis = 1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "dfr_feat"
      ],
      "metadata": {
        "id": "_zwPa4rVk1Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = dfr['sentiment']"
      ],
      "metadata": {
        "id": "7oWY9gwEk1T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "\n",
        "\n",
        "text_counts = cv.fit_transform(dfr['twitts'])"
      ],
      "metadata": {
        "id": "Z01seASlk1Kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_counts.toarray().shape"
      ],
      "metadata": {
        "id": "9-yI3FlIk0_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfr_bow = pd.DataFrame(text_counts.toarray(), columns=cv.get_feature_names())\n",
        "\n",
        "\n",
        "dfr_bow.head(2)"
      ],
      "metadata": {
        "id": "QsCPjQxVk06Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{purple}{\\text{ML Algorithms}}$\n",
        "##$\\color{dark}{\\text{Importing Libraries for ML algorithms}}$"
      ],
      "metadata": {
        "id": "MeqQ17rWlteD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "TsItFgt2k0wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sgd = SGDClassifier(n_jobs=-1, random_state=42, max_iter=200)\n",
        "lgr = LogisticRegression(random_state=42, max_iter=200)\n",
        "lgrcv = LogisticRegressionCV(cv = 2, random_state=42, max_iter=1000)\n",
        "svm = LinearSVC(random_state=42, max_iter=200)\n",
        "rfc = RandomForestClassifier(random_state=42, n_jobs=-1, n_estimators=200)"
      ],
      "metadata": {
        "id": "t3xO47-pk0mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = {'SGD': sgd, 'LGR': lgr, 'LGR-CV': lgrcv, 'SVM': svm, 'RFC': rfc}\n",
        "\n",
        "\n",
        "clf.keys()"
      ],
      "metadata": {
        "id": "sxOJfNHZk0hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#here, we are training our model by defining the function classify.\n",
        "\n",
        "def classify(X, y):\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    X = scaler.fit_transform(X)\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
        "    \n",
        "    for key in clf.keys():\n",
        "        clf[key].fit(X_train, y_train)\n",
        "        y_pred = clf[key].predict(X_test)\n",
        "        ac = accuracy_score(y_test, y_pred)\n",
        "        print(key, \" ---> \", ac)"
      ],
      "metadata": {
        "id": "dxMfRTHUk0X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "classify(dfr_bow, y)"
      ],
      "metadata": {
        "id": "WbWJJ8K_k0Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Manual Feature}}$"
      ],
      "metadata": {
        "id": "7dRHR8K2m7fB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#passing all the manual features.\n",
        "\n",
        "dfr_feat.head(2)"
      ],
      "metadata": {
        "id": "IqM_tdd0kz_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "classify(dfr_feat, y)"
      ],
      "metadata": {
        "id": "9tFUy-23kzz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Manual + Bow}}$"
      ],
      "metadata": {
        "id": "WOviH2EwnfPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#passing all the manual features along with bag of words features.\n",
        "\n",
        "X = dfr_feat.join(dfr_bow)"
      ],
      "metadata": {
        "id": "QJZZdUbLkznU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "classify(X, y)"
      ],
      "metadata": {
        "id": "J15Tmx9nkzYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{TF-IDF}}$"
      ],
      "metadata": {
        "id": "A8hvDiuon9BO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#passing all the manual features along with tfidf features.\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "dfr.shape"
      ],
      "metadata": {
        "id": "5Z7iYVCgn4ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer()\n",
        "X = tfidf.fit_transform(dfr['twitts'])\n"
      ],
      "metadata": {
        "id": "qVL-d17Qn4VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "classify(pd.DataFrame(X.toarray()), y)"
      ],
      "metadata": {
        "id": "ppJTCBMpn4I4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Word2Vec}}$"
      ],
      "metadata": {
        "id": "cKOhjFYOoiQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vec(x):\n",
        "    doc = nlp(x)\n",
        "    return doc.vector.reshape(1, -1)"
      ],
      "metadata": {
        "id": "QM_1IUaHn3_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "dfr['vec'] = dfr['twitts'].apply(lambda x: get_vec(x))"
      ],
      "metadata": {
        "id": "BYoQjqQtn3xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.concatenate(dfr['vec'].to_numpy(), axis = 0)\n",
        "\n",
        "\n",
        "X.shape"
      ],
      "metadata": {
        "id": "g1Xy5dTsn3nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify(pd.DataFrame(X), y)"
      ],
      "metadata": {
        "id": "O9L33aEUn3Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_w2v(x):\n",
        "    for key in clf.keys():\n",
        "        y_pred = clf[key].predict(get_vec(x))\n",
        "        print(key, \"-->\", y_pred)"
      ],
      "metadata": {
        "id": "E6Ov96_2n3MJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_w2v('hi, thanks for pay attention')"
      ],
      "metadata": {
        "id": "mOkOgrk2n2_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y4YNGaPUn2y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dVloec-6n2iJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_W4Rl9ezn1Pz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}